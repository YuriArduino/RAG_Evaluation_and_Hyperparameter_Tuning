# RAG_Evaluation_and_Hyperparameter_Tuning - Optimization Framework
LLM Grounding Leads to More Accurate Contextual Responses -  studies and tests, searching for the best metrics


Um framework completo em Python para **avaliar, comparar e otimizar cientificamente** as estrat√©gias de `Retrieval` em pipelines de RAG (Retrieval-Augmented Generation), utilizando o pr√≥prio LLM para gerar o "ground truth".

---

## A Jornada: De um Exerc√≠cio de RAG a um Laborat√≥rio de MLOps

O que come√ßou como um exerc√≠cio da Aula 2 da Imers√£o Dev (Agentes de IA) da Alura, rapidamente se transformou em uma pergunta muito mais profunda: **como podemos medir, otimizar e, acima de tudo, confiar nos resultados de um sistema de RAG?**

Este reposit√≥rio √© a resposta a essa pergunta. Ele documenta a jornada de refatora√ß√£o de um script simples para uma arquitetura de software modular e test√°vel, culminando em um verdadeiro **laborat√≥rio de MLOps para RAG**.

## O Problema Resolvido

A performance de um sistema RAG √© extremamente sens√≠vel a hiperpar√¢metros como `chunk_size` e `chunk_overlap`. Escolhas ruins podem levar a:
- **Fragmenta√ß√£o de Contexto:** Chunks pequenos que perdem o sentido.
- **Dilui√ß√£o de Informa√ß√£o:** Chunks grandes que escondem a resposta em meio a ru√≠do.
- **Alucina√ß√µes:** O LLM inventa respostas quando o retriever falha.

Este framework foi projetado para substituir a intui√ß√£o por **dados**, permitindo uma tomada de decis√£o de engenharia baseada em evid√™ncias.

## ‚ú® Features

-   üß† **`LLM-derived Ground Truth`:** Usa o Gemini para analisar todos os chunks e criar um "gabarito" objetivo de relev√¢ncia, eliminando a necessidade de anota√ß√£o manual.
-   üî¨ **Benchmarking Automatizado:** Compara sistematicamente m√∫ltiplas estrat√©gias de chunking contra um conjunto de perguntas de teste.
-   ‚öñÔ∏è **M√©tricas Avan√ßadas:** Calcula automaticamente `Precision`, `Recall`, `F1-Score` e `Precision@K` para cada estrat√©gia, fornecendo uma vis√£o completa da performance.
-   üèõÔ∏è **Arquitetura SOLID:** O c√≥digo √© dividido em 15 c√©lulas modulares, com classes especialistas (`RelevanceEvaluator`, `MetricsCalculator`, etc.) que seguem o Princ√≠pio da Responsabilidade √önica.
-   üíæ **Cache Persistente:** Salva os resultados de experimentos caros em disco, permitindo a re-an√°lise instant√¢nea e economizando tempo e custos de API.
-   üìä **Visualiza√ß√£o de Resultados:** Gera um `heatmap` e uma tabela de performance agregada para uma interpreta√ß√£o visual e clara dos resultados.

---

## üì∏ Demonstra√ß√£o dos Resultados

O framework executa os experimentos e gera um dashboard visual para comparar a performance (medida pelo F1-Score) de cada estrat√©gia em todo o conjunto de testes.

![Heatmap de Performance das Estrat√©gias](https://github.com/YuriArduino/RAG_Evaluation_and_Hyperparameter_Tuning/blob/notebook(colab)/Resultado%20dos%20testes%203.png)

---

## üíª Como Usar

O projeto est√° estruturado como um notebook Jupyter (`.ipynb`).

1.  Abra o arquivo `RAG_Evaluation_and_Hyperparameter_Tuning_v2.ipynb` no Google Colab, VS Code, ou Jupyter Lab.
2.  Adicione sua `GEMINI_API_KEY` aos segredos do ambiente (no Colab, clique no √≠cone de chave üîë).
3.  Execute as c√©lulas em ordem, de cima para baixo. A c√©lula final (`main()`) orquestra todo o pipeline de an√°lise.

Os par√¢metros do experimento, como as estrat√©gias de chunking e as perguntas de teste, podem ser facilmente modificados na c√©lula de execu√ß√£o principal para se adequarem a novos documentos ou casos de uso.

---

## üìú Licen√ßa
Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

```
